{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8107130d",
   "metadata": {},
   "source": [
    "## üìä Dataset Statistics\n",
    "\n",
    "| Dataset | Total | Normal | Abnormal |\n",
    "|---------|-------|--------|----------|\n",
    "| **NIH** | 112,120 | 60,361 (53.8%) | 51,759 (46.2%) |\n",
    "| **Pediatric** | 5,856 | 1,583 (27.0%) | 4,273 (73.0%) |\n",
    "| **CheXpert** | 29,031 | 1,123 (3.9%) | 27,908 (96.1%) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23beb31e",
   "metadata": {},
   "source": [
    "---\n",
    "# üîß Setup & Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ba208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check current working directory\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "print(f\"Current working directory: {os.getcwd()}\")\n",
    "print(f\"Python version: {os.sys.version}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a2c2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define project paths (adjust these to your local setup)\n",
    "PROJECT_ROOT = Path(r\"C:\\CAS AML\\M3 Project\")\n",
    "DATA_DIR = PROJECT_ROOT / \"data\" / \"processed\"\n",
    "LABELS_DIR = PROJECT_ROOT / \"data\" / \"labels\"\n",
    "SCRIPTS_DIR = PROJECT_ROOT / \"scripts\"\n",
    "RESULTS_DIR = PROJECT_ROOT / \"Results\"\n",
    "MODELS_DIR = PROJECT_ROOT / \"models\"\n",
    "\n",
    "# Create directories if they don't exist\n",
    "for directory in [DATA_DIR, LABELS_DIR, SCRIPTS_DIR, RESULTS_DIR, MODELS_DIR]:\n",
    "    directory.mkdir(parents=True, exist_ok=True)\n",
    "    print(f\"‚úÖ {directory.name}: {directory}\")\n",
    "\n",
    "# Add scripts to path\n",
    "import sys\n",
    "if str(SCRIPTS_DIR) not in sys.path:\n",
    "    sys.path.insert(0, str(SCRIPTS_DIR))\n",
    "\n",
    "print(f\"\\n‚úÖ Project root: {PROJECT_ROOT}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10c6c29",
   "metadata": {},
   "source": [
    "### Import Libraries (Keras 3.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0182aead",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['KERAS_BACKEND'] = 'torch'  # Options: 'torch', 'tensorflow', 'jax'\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "from sklearn.metrics import (\n",
    "    roc_auc_score, accuracy_score, balanced_accuracy_score,\n",
    "    classification_report, confusion_matrix, roc_curve\n",
    ")\n",
    "import h5py\n",
    "import keras\n",
    "from keras import layers, models, callbacks, optimizers\n",
    "import json\n",
    "from pathlib import Path\n",
    "import gc\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(f\"‚úÖ Keras version: {keras.__version__}\")\n",
    "print(f\"‚úÖ Keras backend: {keras.backend.backend()}\")\n",
    "print(f\"‚úÖ NumPy version: {np.__version__}\")\n",
    "print(f\"‚úÖ Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e36fc7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU availability\n",
    "backend = keras.backend.backend()\n",
    "\n",
    "if backend == 'torch':\n",
    "    import torch\n",
    "    print(f\"PyTorch version: {torch.__version__}\")\n",
    "    print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"CUDA version: {torch.version.cuda}\")\n",
    "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "        print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "elif backend == 'tensorflow':\n",
    "    import tensorflow as tf\n",
    "    print(f\"TensorFlow version: {tf.__version__}\")\n",
    "    print(f\"GPU devices: {tf.config.list_physical_devices('GPU')}\")\n",
    "elif backend == 'jax':\n",
    "    import jax\n",
    "    print(f\"JAX version: {jax.__version__}\")\n",
    "    print(f\"JAX devices: {jax.devices()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e50e1ac",
   "metadata": {},
   "source": [
    "---\n",
    "# üìÅ Phase 1: Baseline Statistical Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "babd6854",
   "metadata": {},
   "source": [
    "## Phase 1a: All Data Comparison\n",
    "\n",
    "**Goal:** Quantify baseline distribution shift using all images (mixed pathologies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3ea086a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test images from all datasets\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 1a: LOADING TEST DATA (ALL IMAGES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for dataset_name in ['nih', 'pediatric', 'chexpert']:\n",
    "    h5_path = DATA_DIR / dataset_name / 'test_all.h5'\n",
    "    \n",
    "    if not h5_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  {dataset_name}: File not found - {h5_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüìÇ Loading {dataset_name}...\")\n",
    "    \n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        images = f['images'][:]\n",
    "        labels = f['labels'][:]\n",
    "        \n",
    "    datasets[dataset_name] = {\n",
    "        'images': images,\n",
    "        'labels': labels,\n",
    "        'n_samples': len(images)\n",
    "    }\n",
    "    \n",
    "    print(f\"   Shape: {images.shape}\")\n",
    "    print(f\"   Samples: {len(images):,}\")\n",
    "    print(f\"   Memory: {images.nbytes / 1024**2:.1f} MB\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(datasets)} datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2449d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute pixel statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPUTING PIXEL STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stats_1a = []\n",
    "\n",
    "for name, data in datasets.items():\n",
    "    images = data['images']\n",
    "    \n",
    "    stat = {\n",
    "        'Dataset': name.upper(),\n",
    "        'Mean': images.mean(),\n",
    "        'Std': images.std(),\n",
    "        'Min': images.min(),\n",
    "        'Max': images.max(),\n",
    "        'Median': np.median(images),\n",
    "        'Q25': np.percentile(images, 25),\n",
    "        'Q75': np.percentile(images, 75)\n",
    "    }\n",
    "    stats_1a.append(stat)\n",
    "    \n",
    "    print(f\"\\n{name.upper()}:\")\n",
    "    print(f\"  Mean:   {stat['Mean']:.4f}\")\n",
    "    print(f\"  Std:    {stat['Std']:.4f}\")\n",
    "    print(f\"  Range:  [{stat['Min']:.4f}, {stat['Max']:.4f}]\")\n",
    "    print(f\"  Median: {stat['Median']:.4f}\")\n",
    "\n",
    "df_stats_1a = pd.DataFrame(stats_1a)\n",
    "print(\"\\n\" + df_stats_1a.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039c38ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute JS Divergence\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "\n",
    "def compute_js_divergence(images1, images2, bins=100):\n",
    "    \"\"\"Compute Jensen-Shannon divergence between two image distributions\"\"\"\n",
    "    hist1, _ = np.histogram(images1.flatten(), bins=bins, range=(0, 1), density=True)\n",
    "    hist2, _ = np.histogram(images2.flatten(), bins=bins, range=(0, 1), density=True)\n",
    "    \n",
    "    # Normalize to probability distributions\n",
    "    hist1 = hist1 / hist1.sum()\n",
    "    hist2 = hist2 / hist2.sum()\n",
    "    \n",
    "    return jensenshannon(hist1, hist2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPUTING JENSEN-SHANNON DIVERGENCE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "js_results_1a = []\n",
    "dataset_names = list(datasets.keys())\n",
    "\n",
    "for i, name1 in enumerate(dataset_names):\n",
    "    for name2 in dataset_names[i+1:]:\n",
    "        js_div = compute_js_divergence(\n",
    "            datasets[name1]['images'],\n",
    "            datasets[name2]['images']\n",
    "        )\n",
    "        \n",
    "        js_results_1a.append({\n",
    "            'Dataset1': name1.upper(),\n",
    "            'Dataset2': name2.upper(),\n",
    "            'JS_Divergence': js_div\n",
    "        })\n",
    "        \n",
    "        print(f\"  {name1.upper():10s} vs {name2.upper():10s}: {js_div:.4f}\")\n",
    "\n",
    "df_js_1a = pd.DataFrame(js_results_1a)\n",
    "\n",
    "# Save results\n",
    "phase1a_dir = RESULTS_DIR / 'phase1a'\n",
    "phase1a_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_stats_1a.to_csv(phase1a_dir / 'phase1a_statistics.csv', index=False)\n",
    "df_js_1a.to_csv(phase1a_dir / 'phase1a_js_divergence.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to {phase1a_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9aba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize JS Divergence\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "comparisons = [f\"{row['Dataset1']} vs\\n{row['Dataset2']}\" for _, row in df_js_1a.iterrows()]\n",
    "values = df_js_1a['JS_Divergence'].values\n",
    "\n",
    "bars = ax.bar(comparisons, values, color=['#FF6B6B', '#4ECDC4', '#45B7D1'], alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, values):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.4f}',\n",
    "            ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('JS Divergence', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Phase 1a: Distribution Shift (All Images)', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.set_ylim(0, max(values) * 1.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(phase1a_dir / 'phase1a_js_divergence.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6576190d",
   "metadata": {},
   "source": [
    "## Phase 1b: Normals-Only Comparison\n",
    "\n",
    "**Goal:** Isolate institutional/demographic shift by controlling for pathology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d110744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load normal images only\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 1b: LOADING NORMAL IMAGES ONLY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "datasets_normal = {}\n",
    "\n",
    "for dataset_name in ['nih', 'pediatric', 'chexpert']:\n",
    "    h5_path = DATA_DIR / dataset_name / 'test_normals.h5'\n",
    "    \n",
    "    if not h5_path.exists():\n",
    "        print(f\"‚ö†Ô∏è  {dataset_name}: File not found - {h5_path}\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\nüìÇ Loading {dataset_name} normals...\")\n",
    "    \n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        images = f['images'][:]\n",
    "        labels = f['labels'][:]\n",
    "        \n",
    "    datasets_normal[dataset_name] = {\n",
    "        'images': images,\n",
    "        'labels': labels,\n",
    "        'n_samples': len(images)\n",
    "    }\n",
    "    \n",
    "    print(f\"   Shape: {images.shape}\")\n",
    "    print(f\"   Samples: {len(images):,}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Loaded {len(datasets_normal)} datasets (normals only)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b26350",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistics and JS divergence for normals\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPUTING STATISTICS FOR NORMALS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "stats_1b = []\n",
    "js_results_1b = []\n",
    "\n",
    "# Statistics\n",
    "for name, data in datasets_normal.items():\n",
    "    images = data['images']\n",
    "    stat = {\n",
    "        'Dataset': name.upper(),\n",
    "        'Mean': images.mean(),\n",
    "        'Std': images.std(),\n",
    "        'Min': images.min(),\n",
    "        'Max': images.max(),\n",
    "        'Median': np.median(images)\n",
    "    }\n",
    "    stats_1b.append(stat)\n",
    "\n",
    "# JS Divergence\n",
    "dataset_names = list(datasets_normal.keys())\n",
    "for i, name1 in enumerate(dataset_names):\n",
    "    for name2 in dataset_names[i+1:]:\n",
    "        js_div = compute_js_divergence(\n",
    "            datasets_normal[name1]['images'],\n",
    "            datasets_normal[name2]['images']\n",
    "        )\n",
    "        \n",
    "        js_results_1b.append({\n",
    "            'Dataset1': name1.upper(),\n",
    "            'Dataset2': name2.upper(),\n",
    "            'JS_Divergence': js_div\n",
    "        })\n",
    "        \n",
    "        print(f\"  {name1.upper():10s} vs {name2.upper():10s}: {js_div:.4f}\")\n",
    "\n",
    "df_stats_1b = pd.DataFrame(stats_1b)\n",
    "df_js_1b = pd.DataFrame(js_results_1b)\n",
    "\n",
    "# Save results\n",
    "phase1b_dir = RESULTS_DIR / 'phase1b'\n",
    "phase1b_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_stats_1b.to_csv(phase1b_dir / 'phase1b_statistics.csv', index=False)\n",
    "df_js_1b.to_csv(phase1b_dir / 'phase1b_js_divergence.csv', index=False)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to {phase1b_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32362373",
   "metadata": {},
   "source": [
    "---\n",
    "# üìÅ Phase 2: Autoencoder Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec152a8",
   "metadata": {},
   "source": [
    "## Phase 2a: NIH_Full Autoencoder\n",
    "\n",
    "**Data:** All NIH images (normals + abnormals)  \n",
    "**Architecture:** Convolutional autoencoder with 256-dim latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413308e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build autoencoder architecture\n",
    "def build_autoencoder(input_shape=(224, 224, 1), latent_dim=256):\n",
    "    \"\"\"\n",
    "    Build convolutional autoencoder\n",
    "    \n",
    "    Args:\n",
    "        input_shape: Input image shape\n",
    "        latent_dim: Latent space dimension\n",
    "        \n",
    "    Returns:\n",
    "        encoder, decoder, autoencoder models\n",
    "    \"\"\"\n",
    "    \n",
    "    # ENCODER\n",
    "    encoder_input = layers.Input(shape=input_shape, name='encoder_input')\n",
    "    \n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same', name='enc_conv1')(encoder_input)\n",
    "    x = layers.MaxPooling2D(2, padding='same', name='enc_pool1')(x)\n",
    "    \n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same', name='enc_conv2')(x)\n",
    "    x = layers.MaxPooling2D(2, padding='same', name='enc_pool2')(x)\n",
    "    \n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same', name='enc_conv3')(x)\n",
    "    x = layers.MaxPooling2D(2, padding='same', name='enc_pool3')(x)\n",
    "    \n",
    "    x = layers.Conv2D(256, 3, activation='relu', padding='same', name='enc_conv4')(x)\n",
    "    x = layers.MaxPooling2D(2, padding='same', name='enc_pool4')(x)\n",
    "    \n",
    "    x = layers.Flatten(name='enc_flatten')(x)\n",
    "    encoder_output = layers.Dense(latent_dim, activation='relu', name='latent')(x)\n",
    "    \n",
    "    encoder = models.Model(encoder_input, encoder_output, name='encoder')\n",
    "    \n",
    "    # DECODER\n",
    "    decoder_input = layers.Input(shape=(latent_dim,), name='decoder_input')\n",
    "    \n",
    "    x = layers.Dense(14 * 14 * 256, activation='relu', name='dec_dense')(decoder_input)\n",
    "    x = layers.Reshape((14, 14, 256), name='dec_reshape')(x)\n",
    "    \n",
    "    x = layers.Conv2DTranspose(128, 3, activation='relu', strides=2, padding='same', name='dec_conv1')(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same', name='dec_conv2')(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same', name='dec_conv3')(x)\n",
    "    x = layers.Conv2DTranspose(16, 3, activation='relu', strides=2, padding='same', name='dec_conv4')(x)\n",
    "    \n",
    "    decoder_output = layers.Conv2D(1, 3, activation='sigmoid', padding='same', name='decoder_output')(x)\n",
    "    \n",
    "    decoder = models.Model(decoder_input, decoder_output, name='decoder')\n",
    "    \n",
    "    # AUTOENCODER\n",
    "    autoencoder_input = layers.Input(shape=input_shape, name='autoencoder_input')\n",
    "    encoded = encoder(autoencoder_input)\n",
    "    decoded = decoder(encoded)\n",
    "    autoencoder = models.Model(autoencoder_input, decoded, name='autoencoder')\n",
    "    \n",
    "    return encoder, decoder, autoencoder\n",
    "\n",
    "# Build models\n",
    "print(\"Building autoencoder architecture...\")\n",
    "encoder_2a, decoder_2a, autoencoder_2a = build_autoencoder()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ENCODER ARCHITECTURE\")\n",
    "print(\"=\"*80)\n",
    "encoder_2a.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DECODER ARCHITECTURE\")\n",
    "print(\"=\"*80)\n",
    "decoder_2a.summary()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AUTOENCODER ARCHITECTURE\")\n",
    "print(\"=\"*80)\n",
    "autoencoder_2a.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de90db49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load NIH training data\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING NIH TRAINING DATA (ALL IMAGES)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load train and validation\n",
    "train_path = DATA_DIR / 'nih' / 'train_all.h5'\n",
    "val_path = DATA_DIR / 'nih' / 'val_all.h5'\n",
    "\n",
    "with h5py.File(train_path, 'r') as f:\n",
    "    X_train = f['images'][:]\n",
    "    print(f\"‚úÖ Training: {X_train.shape}\")\n",
    "\n",
    "with h5py.File(val_path, 'r') as f:\n",
    "    X_val = f['images'][:]\n",
    "    print(f\"‚úÖ Validation: {X_val.shape}\")\n",
    "\n",
    "# Normalize to [0, 1] if needed\n",
    "if X_train.max() > 1.0:\n",
    "    X_train = X_train / 255.0\n",
    "    X_val = X_val / 255.0\n",
    "    print(\"‚úÖ Normalized to [0, 1]\")\n",
    "\n",
    "print(f\"\\nTraining memory: {X_train.nbytes / 1024**3:.2f} GB\")\n",
    "print(f\"Validation memory: {X_val.nbytes / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2ad09c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train autoencoder\n",
    "print(\"=\"*80)\n",
    "print(\"COMPILING AND TRAINING AUTOENCODER (PHASE 2a)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compile\n",
    "autoencoder_2a.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "phase2a_dir = RESULTS_DIR / 'phase2a'\n",
    "phase2a_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "checkpoint_path = str(phase2a_dir / 'autoencoder_best.keras')\n",
    "\n",
    "callbacks_list = [\n",
    "    callbacks.ModelCheckpoint(\n",
    "        checkpoint_path,\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train\n",
    "print(\"\\nStarting training...\")\n",
    "history_2a = autoencoder_2a.fit(\n",
    "    X_train, X_train,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val, X_val),\n",
    "    callbacks=callbacks_list,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c57932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save models and history\n",
    "print(\"Saving models...\")\n",
    "\n",
    "autoencoder_2a.save(phase2a_dir / 'autoencoder_final.keras')\n",
    "encoder_2a.save(phase2a_dir / 'encoder.keras')\n",
    "decoder_2a.save(phase2a_dir / 'decoder.keras')\n",
    "\n",
    "# Save training history\n",
    "history_dict = {\n",
    "    'loss': [float(x) for x in history_2a.history['loss']],\n",
    "    'val_loss': [float(x) for x in history_2a.history['val_loss']],\n",
    "    'mae': [float(x) for x in history_2a.history.get('mae', [])],\n",
    "    'val_mae': [float(x) for x in history_2a.history.get('val_mae', [])]\n",
    "}\n",
    "\n",
    "with open(phase2a_dir / 'training_history.json', 'w') as f:\n",
    "    json.dump(history_dict, f, indent=2)\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'dataset': 'NIH_Full',\n",
    "    'train_samples': len(X_train),\n",
    "    'val_samples': len(X_val),\n",
    "    'epochs_trained': len(history_2a.history['loss']),\n",
    "    'best_val_loss': float(min(history_2a.history['val_loss'])),\n",
    "    'architecture': 'ConvAutoencoder',\n",
    "    'latent_dim': 256,\n",
    "    'backend': keras.backend.backend()\n",
    "}\n",
    "\n",
    "with open(phase2a_dir / 'metadata.json', 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"‚úÖ All files saved to {phase2a_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df5b6285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training curves\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss\n",
    "axes[0].plot(history_2a.history['loss'], label='Train Loss', linewidth=2)\n",
    "axes[0].plot(history_2a.history['val_loss'], label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0].set_ylabel('MSE Loss', fontsize=12)\n",
    "axes[0].set_title('Phase 2a: Training Loss', fontsize=14, fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# MAE\n",
    "if 'mae' in history_2a.history:\n",
    "    axes[1].plot(history_2a.history['mae'], label='Train MAE', linewidth=2)\n",
    "    axes[1].plot(history_2a.history['val_mae'], label='Val MAE', linewidth=2)\n",
    "    axes[1].set_xlabel('Epoch', fontsize=12)\n",
    "    axes[1].set_ylabel('MAE', fontsize=12)\n",
    "    axes[1].set_title('Phase 2a: Mean Absolute Error', fontsize=14, fontweight='bold')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(phase2a_dir / 'training_curves.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Training curves saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a8260e",
   "metadata": {},
   "source": [
    "## Phase 2b: NIH_Normal Autoencoder\n",
    "\n",
    "**Data:** Only normal NIH images  \n",
    "**Purpose:** Learn \"normal\" appearance to detect distribution shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfb5be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build and train Phase 2b autoencoder (normals only)\n",
    "# [Similar code to Phase 2a, but using *_normals.h5 files]\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 2b: NIH_NORMAL AUTOENCODER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load normal images only\n",
    "train_normal_path = DATA_DIR / 'nih' / 'train_normals.h5'\n",
    "val_normal_path = DATA_DIR / 'nih' / 'val_normals.h5'\n",
    "\n",
    "with h5py.File(train_normal_path, 'r') as f:\n",
    "    X_train_normal = f['images'][:]\n",
    "    print(f\"‚úÖ Training (normals): {X_train_normal.shape}\")\n",
    "\n",
    "with h5py.File(val_normal_path, 'r') as f:\n",
    "    X_val_normal = f['images'][:]\n",
    "    print(f\"‚úÖ Validation (normals): {X_val_normal.shape}\")\n",
    "\n",
    "# Normalize\n",
    "if X_train_normal.max() > 1.0:\n",
    "    X_train_normal = X_train_normal / 255.0\n",
    "    X_val_normal = X_val_normal / 255.0\n",
    "\n",
    "# Build new autoencoder\n",
    "encoder_2b, decoder_2b, autoencoder_2b = build_autoencoder()\n",
    "\n",
    "# Compile\n",
    "autoencoder_2b.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "phase2b_dir = RESULTS_DIR / 'phase2b'\n",
    "phase2b_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks_2b = [\n",
    "    callbacks.ModelCheckpoint(\n",
    "        str(phase2b_dir / 'autoencoder_best.keras'),\n",
    "        monitor='val_loss',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train\n",
    "print(\"\\nStarting training (normals only)...\")\n",
    "history_2b = autoencoder_2b.fit(\n",
    "    X_train_normal, X_train_normal,\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_normal, X_val_normal),\n",
    "    callbacks=callbacks_2b,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Save models\n",
    "autoencoder_2b.save(phase2b_dir / 'autoencoder_final.keras')\n",
    "encoder_2b.save(phase2b_dir / 'encoder.keras')\n",
    "decoder_2b.save(phase2b_dir / 'decoder.keras')\n",
    "\n",
    "print(\"\\n‚úÖ Phase 2b complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ae7f9f",
   "metadata": {},
   "source": [
    "---\n",
    "# üìÅ Phase 3: Reconstruction Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ba845f",
   "metadata": {},
   "source": [
    "## Phase 3a: NIH_Full Autoencoder on All Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1440515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute reconstruction errors\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 3a: RECONSTRUCTION ERROR ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load best autoencoder\n",
    "autoencoder_2a_best = keras.models.load_model(phase2a_dir / 'autoencoder_best.keras')\n",
    "\n",
    "results_3a = {}\n",
    "\n",
    "for dataset_name in ['nih', 'pediatric', 'chexpert']:\n",
    "    print(f\"\\nüìä Processing {dataset_name.upper()}...\")\n",
    "    \n",
    "    # Load test images\n",
    "    test_path = DATA_DIR / dataset_name / 'test_all.h5'\n",
    "    \n",
    "    with h5py.File(test_path, 'r') as f:\n",
    "        X_test = f['images'][:]\n",
    "    \n",
    "    if X_test.max() > 1.0:\n",
    "        X_test = X_test / 255.0\n",
    "    \n",
    "    # Reconstruct\n",
    "    X_reconstructed = autoencoder_2a_best.predict(X_test, batch_size=32, verbose=0)\n",
    "    \n",
    "    # Compute per-image MSE\n",
    "    errors = np.mean((X_test - X_reconstructed) ** 2, axis=(1, 2, 3))\n",
    "    \n",
    "    results_3a[dataset_name] = {\n",
    "        'mean': float(errors.mean()),\n",
    "        'std': float(errors.std()),\n",
    "        'median': float(np.median(errors)),\n",
    "        'min': float(errors.min()),\n",
    "        'max': float(errors.max()),\n",
    "        'errors': errors\n",
    "    }\n",
    "    \n",
    "    print(f\"   Mean error: {errors.mean():.6f}\")\n",
    "    print(f\"   Std:        {errors.std():.6f}\")\n",
    "    print(f\"   Range:      [{errors.min():.6f}, {errors.max():.6f}]\")\n",
    "\n",
    "# Save results\n",
    "phase3a_dir = RESULTS_DIR / 'phase3a'\n",
    "phase3a_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "summary_3a = pd.DataFrame([\n",
    "    {\n",
    "        'Dataset': name.upper(),\n",
    "        'Mean_Error': results_3a[name]['mean'],\n",
    "        'Std_Error': results_3a[name]['std'],\n",
    "        'Median_Error': results_3a[name]['median']\n",
    "    }\n",
    "    for name in results_3a.keys()\n",
    "])\n",
    "\n",
    "summary_3a.to_csv(phase3a_dir / 'phase3a_summary.csv', index=False)\n",
    "\n",
    "with open(phase3a_dir / 'phase3a_statistics.json', 'w') as f:\n",
    "    json.dump({k: {kk: vv for kk, vv in v.items() if kk != 'errors'} \n",
    "               for k, v in results_3a.items()}, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to {phase3a_dir}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac39967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize reconstruction errors\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "dataset_order = ['nih', 'pediatric', 'chexpert']\n",
    "means = [results_3a[d]['mean'] for d in dataset_order]\n",
    "labels = [d.upper() for d in dataset_order]\n",
    "\n",
    "bars = ax.bar(labels, means, color=['#4ECDC4', '#FF6B6B', '#45B7D1'], alpha=0.8, edgecolor='black')\n",
    "\n",
    "for bar, val in zip(bars, means):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{val:.6f}',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_ylabel('Mean Reconstruction Error (MSE)', fontsize=14, fontweight='bold')\n",
    "ax.set_title('Phase 3a: Reconstruction Error by Dataset', fontsize=16, fontweight='bold', pad=20)\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(phase3a_dir / 'phase3a_reconstruction_errors.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"‚úÖ Visualization saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72833e8a",
   "metadata": {},
   "source": [
    "---\n",
    "# üìÅ Phase 4: Classifier Training & Evaluation\n",
    "\n",
    "**Model:** DenseNet121-based binary classifier (Normal vs Abnormal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32459d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build classifier (DenseNet121 base)\n",
    "from keras.applications import DenseNet121\n",
    "\n",
    "def build_classifier(input_shape=(224, 224, 3)):\n",
    "    \"\"\"\n",
    "    Build binary classifier using DenseNet121 backbone\n",
    "    \"\"\"\n",
    "    base_model = DenseNet121(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_shape=input_shape,\n",
    "        pooling='avg'\n",
    "    )\n",
    "    \n",
    "    # Freeze early layers\n",
    "    for layer in base_model.layers[:100]:\n",
    "        layer.trainable = False\n",
    "    \n",
    "    # Build classifier head\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    x = base_model(inputs, training=False)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    x = layers.Dropout(0.3)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    \n",
    "    model = models.Model(inputs, outputs, name='chest_xray_classifier')\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"Building classifier...\")\n",
    "classifier = build_classifier()\n",
    "classifier.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78de74fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare NIH data for classifier training\n",
    "print(\"=\"*80)\n",
    "print(\"LOADING NIH DATA FOR CLASSIFIER TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load train/val/test\n",
    "with h5py.File(DATA_DIR / 'nih' / 'train_all.h5', 'r') as f:\n",
    "    X_train_clf = f['images'][:]\n",
    "    y_train_clf = f['labels'][:, 0]  # Column 0 = 'No Finding' (1=normal, 0=abnormal)\n",
    "    # Invert labels: 0=normal, 1=abnormal\n",
    "    y_train_clf = 1 - y_train_clf\n",
    "\n",
    "with h5py.File(DATA_DIR / 'nih' / 'val_all.h5', 'r') as f:\n",
    "    X_val_clf = f['images'][:]\n",
    "    y_val_clf = 1 - f['labels'][:, 0]\n",
    "\n",
    "with h5py.File(DATA_DIR / 'nih' / 'test_all.h5', 'r') as f:\n",
    "    X_test_clf = f['images'][:]\n",
    "    y_test_clf = 1 - f['labels'][:, 0]\n",
    "\n",
    "# Normalize\n",
    "if X_train_clf.max() > 1.0:\n",
    "    X_train_clf = X_train_clf / 255.0\n",
    "    X_val_clf = X_val_clf / 255.0\n",
    "    X_test_clf = X_test_clf / 255.0\n",
    "\n",
    "# Convert grayscale to RGB (DenseNet expects 3 channels)\n",
    "X_train_clf = np.repeat(X_train_clf, 3, axis=-1)\n",
    "X_val_clf = np.repeat(X_val_clf, 3, axis=-1)\n",
    "X_test_clf = np.repeat(X_test_clf, 3, axis=-1)\n",
    "\n",
    "print(f\"‚úÖ Training: {X_train_clf.shape}, Labels: {y_train_clf.shape}\")\n",
    "print(f\"‚úÖ Validation: {X_val_clf.shape}, Labels: {y_val_clf.shape}\")\n",
    "print(f\"‚úÖ Test: {X_test_clf.shape}, Labels: {y_test_clf.shape}\")\n",
    "print(f\"\\nClass distribution (train): {np.bincount(y_train_clf.astype(int))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5159a096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile and train classifier\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING CLASSIFIER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Compute class weights for imbalanced data\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "class_weights_array = compute_class_weight(\n",
    "    'balanced',\n",
    "    classes=np.unique(y_train_clf),\n",
    "    y=y_train_clf\n",
    ")\n",
    "class_weights = {i: class_weights_array[i] for i in range(len(class_weights_array))}\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "\n",
    "# Compile\n",
    "classifier.compile(\n",
    "    optimizer=optimizers.Adam(learning_rate=0.0001),\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy', keras.metrics.AUC(name='auc')]\n",
    ")\n",
    "\n",
    "# Setup callbacks\n",
    "phase4_dir = RESULTS_DIR / 'phase4'\n",
    "phase4_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "callbacks_4 = [\n",
    "    callbacks.ModelCheckpoint(\n",
    "        str(phase4_dir / 'classifier_best.keras'),\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.EarlyStopping(\n",
    "        monitor='val_auc',\n",
    "        mode='max',\n",
    "        patience=10,\n",
    "        restore_best_weights=True,\n",
    "        verbose=1\n",
    "    ),\n",
    "    callbacks.ReduceLROnPlateau(\n",
    "        monitor='val_loss',\n",
    "        factor=0.5,\n",
    "        patience=5,\n",
    "        min_lr=1e-7,\n",
    "        verbose=1\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train\n",
    "print(\"\\nStarting training...\")\n",
    "history_4 = classifier.fit(\n",
    "    X_train_clf, y_train_clf,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_clf, y_val_clf),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks_4,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\n‚úÖ Classifier training complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad269de",
   "metadata": {},
   "source": [
    "---\n",
    "# üìÅ Phase 5: Correlation Analysis\n",
    "\n",
    "**Goal:** Test if reconstruction error predicts classifier performance degradation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c865a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute correlations between reconstruction error and classifier performance\n",
    "print(\"=\"*80)\n",
    "print(\"PHASE 5: CORRELATION ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load Phase 3 and Phase 4 results\n",
    "# Combine reconstruction errors with classifier metrics\n",
    "# Compute Pearson and Spearman correlations\n",
    "\n",
    "phase5_dir = RESULTS_DIR / 'phase5'\n",
    "phase5_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "correlation_results = {\n",
    "    'reconstruction_error': [],\n",
    "    'balanced_accuracy': [],\n",
    "    'auc': []\n",
    "}\n",
    "\n",
    "# [Add correlation computation code here]\n",
    "\n",
    "print(\"\\n‚úÖ Correlation analysis complete!\")\n",
    "print(f\"Results saved to {phase5_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c09445",
   "metadata": {},
   "source": [
    "---\n",
    "# üìä Summary & Conclusions\n",
    "\n",
    "All phases complete! Check the `Results/` directory for:\n",
    "- Phase 1: Statistical analysis and JS divergence\n",
    "- Phase 2: Trained autoencoders\n",
    "- Phase 3: Reconstruction error analysis\n",
    "- Phase 4: Classifier evaluation\n",
    "- Phase 5: Correlation results"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
